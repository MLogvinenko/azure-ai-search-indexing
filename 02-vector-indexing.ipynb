{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Prerequisites\n",
    "\n",
    "Before executing this notebook, make sure you have properly set up your Azure Services, created your Conda environment, and configured your environment variables as per the instructions provided in the [README.md](README.md) file.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "This notebook guides you through the following sections:\n",
    "\n",
    "1. [**Create an Azure Cognitive Search Index**](#create-index): This index will store the content from a document hosted on SharePoint Online.\n",
    "\n",
    "2. [**Initialize the `client_extractor` client**](#init-client): This client manages the connection to a SharePoint site through the Microsoft Graph REST API and retrieves the Site ID for the site.\n",
    "\n",
    "3. [**Download and Process Content and Metadata**](#download-process): The `client_extractor` client provides several methods for this:\n",
    "    - Download and process all `.docx` and `.pdf` files from a SharePoint site.\n",
    "    - Download and process only `.docx` files from a specific SharePoint site that were modified or uploaded in the last 60 minutes.\n",
    "    - Download and process files from a specific folder within a SharePoint site.\n",
    "    - Download and process a specific file within a SharePoint site.\n",
    "\n",
    "4. [**Ingest into Azure AI Search Index**](#ingest-index): The extracted content and metadata are ingested into the Azure AI Search Index for easy retrieval and search.\n",
    "\n",
    "For more details, refer to the following resources:\n",
    "- [Quickstart: Register an app with the Azure AD v2.0 endpoint](https://learn.microsoft.com/en-us/azure/active-directory/develop/console-app-quickstart?pivots=devlang-python)\n",
    "- [Create a Demo SharePoint Online Environment](https://cdx.transform.microsoft.com/) (Note: To use this, you need to either be a Microsoft Employee or part of the Microsoft Partner Program: [Microsoft Partner Program](https://partner.microsoft.com/dashboard/account/v3/enrollment/introduction/partnership))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "#### Configure Environment Variables \n",
    "\n",
    "Before running this notebook, you must configure certain environment variables. We will now use environment variables to store our configuration. This is a more secure practice as it prevents sensitive data from being accidentally committed and pushed to version control systems.\n",
    "\n",
    "Create a `.env` file in your project root (use the provided `.env.sample` as a template) and add the following variables:\n",
    "\n",
    "```env\n",
    "# Azure AI Search Service Configuration\n",
    "AZURE_AI_SEARCH_SERVICE_ENDPOINT=\"[Your Azure Search Service Endpoint]\"\n",
    "AZURE_SEARCH_ADMIN_KEY=\"[Your Azure Search Index Name]\"\n",
    "\n",
    "#Azure Open API Configuration\n",
    "AZURE_OPENAI_API_KEY='[Your OpenAI API Key]'\n",
    "AZURE_OPENAI_ENDPOINT='[Your OpenAI Endpoint]'\n",
    "AZURE_OPENAI_API_VERSION='[Your Azure OpenAI API Version]'\n",
    "\n",
    "#Azure Open API Configuration\n",
    "AZURE_STORAGE_CONNECTION_STRING='[Your Azure Storage Connection String]'\n",
    "```\n",
    "\n",
    "Replace the placeholders (e.g., [Your Azure Search Service Endpoint]) with your actual values.\n",
    "\n",
    "- `AZURE_AI_SEARCH_SERVICE_ENDPOINT` and `AZURE_SEARCH_ADMIN_KEY` are used to configure the Azure AI Search service.\n",
    "- `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, and `AZURE_OPENAI_API_VERSION` are used to configure the Azure OpenAI service.\n",
    "- `AZURE_STORAGE_CONNECTION_STRING` is used to configure the Azure Storage service.\n",
    "```\n",
    "\n",
    "> ðŸ“Œ **Note**\n",
    "> Remember not to commit the .env file to your version control system. Add it to your .gitignore file to prevent it from being tracked.\n",
    "\n",
    "#### Setting Up Conda Environment and Configuring VSCode for Jupyter Notebooks (Optional)\n",
    "\n",
    "Follow these steps to create a Conda environment and set up your VSCode for running Jupyter Notebooks:\n",
    "\n",
    "##### Create Conda Environment from the Repository\n",
    "\n",
    "> Instructions for Windows users: \n",
    "\n",
    "1. **Create the Conda Environment**:\n",
    "   - In your terminal or command line, navigate to the repository directory.\n",
    "   - Execute the following command to create the Conda environment using the `environment.yml` file:\n",
    "     ```bash\n",
    "     conda env create -f environment.yml\n",
    "     ```\n",
    "   - This command creates a Conda environment as defined in `environment.yml`.\n",
    "\n",
    "2. **Activating the Environment**:\n",
    "   - After creation, activate the new Conda environment by using:\n",
    "     ```bash\n",
    "     conda activate sharepoint-indexing\n",
    "     ```\n",
    "\n",
    "> Instructions for Linux users (or Windows users with WSL or other linux setup): \n",
    "\n",
    "1. **Use `make` to Create the Conda Environment**:\n",
    "   - In your terminal or command line, navigate to the repository directory and look at the Makefile.\n",
    "   - Execute the `make` command specified below to create the Conda environment using the `environment.yml` file:\n",
    "     ```bash\n",
    "     make create_conda_env\n",
    "     ```\n",
    "\n",
    "2. **Activating the Environment**:\n",
    "   - After creation, activate the new Conda environment by using:\n",
    "     ```bash\n",
    "     conda activate sharepoint-indexing\n",
    "     ```\n",
    "\n",
    "##### Configure VSCode for Jupyter Notebooks\n",
    "\n",
    "1. **Install Required Extensions**:\n",
    "   - Download and install the `Python` and `Jupyter` extensions for VSCode. These extensions provide support for running and editing Jupyter Notebooks within VSCode.\n",
    "\n",
    "2. **Open the Notebook**:\n",
    "   - Open the Jupyter Notebook file (`01-indexing-content.ipynb`) in VSCode.\n",
    "\n",
    "3. **Attach Kernel to VSCode**:\n",
    "   - After creating the Conda environment, it should be available in the kernel selection dropdown. This dropdown is located in the top-right corner of the VSCode interface.\n",
    "   - Select your newly created environment (`sharepoint-indexing`) from the dropdown. This sets it as the kernel for running your Jupyter Notebooks.\n",
    "\n",
    "4. **Run the Notebook**:\n",
    "   - Once the kernel is attached, you can run the notebook by clicking on the \"Run All\" button in the top menu, or by running each cell individually.\n",
    "\n",
    "\n",
    "By following these steps, you'll establish a dedicated Conda environment for your project and configure VSCode to run Jupyter Notebooks efficiently. This environment will include all the necessary dependencies specified in your `environment.yml` file. If you wish to add more packages or change versions, please use `pip install` in a notebook cell or in the terminal after activating the environment, and then restart the kernel. The changes should be automatically applied after the session restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-langchain-azureai-search\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r\"C:\\Users\\pablosal\\Desktop\\gbbai-langchain-azureai-search\"  # change your directory here\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize `TextChunkingIndexing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 01:51:03,965 - micro - MainProcess - INFO     Loading OpenAIEmbeddings object with model, deployment foundational-ada, and chunk size 1000 (langchain_integration_azureai.py:load_embedding_model:113)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 01:51:05,605 - micro - MainProcess - INFO     AzureOpenAIEmbeddings object created successfully. (langchain_integration_azureai.py:load_embedding_model:124)\n"
     ]
    }
   ],
   "source": [
    "# Import the TextChunkingIndexing class from the langchain_integration module\n",
    "from src.gbb_ai.langchain_integration_azureai import TextChunkingIndexing\n",
    "\n",
    "# Create an instance of the TextChunkingIndexing class\n",
    "gbb_ai_client = TextChunkingIndexing()\n",
    "\n",
    "# load the environment variables from the .env file\n",
    "gbb_ai_client.load_environment_variables_from_env_file()\n",
    "\n",
    "# Specify the name of the deployment in Azure AI Services\n",
    "DEPLOYMENT_NAME = \"foundational-ada\"\n",
    "\n",
    "# Load the embedding model associated with the specified deployment\n",
    "embedding_model = gbb_ai_client.load_embedding_model(azure_deployment=DEPLOYMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create/Load the Azure AI Search Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the Azure Search index\n",
    "# This is the index where your data is stored in Azure Search\n",
    "INDEX_NAME = \"index-test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Search Fields with Azure AI\n",
    "\n",
    "In this section, we define the fields that will be used for indexing and searching in Azure AI. These fields represent the different pieces of data that Azure AI will use to understand and categorize the information, enabling more efficient and accurate search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchFieldDataType,\n",
    "    SearchField,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticSettings,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    ")\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchField(\n",
    "        name=\"content_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=len(embedding_model.embed_query(\"Text\")),\n",
    "        vector_search_configuration=\"default\",\n",
    "    ),\n",
    "    SearchableField(name=\"metadata\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SimpleField(name=\"source\", type=SearchFieldDataType.String, filterable=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Semantic Search Parameters\n",
    "\n",
    "In this section, we set up the configuration for semantic search. Semantic search is a type of information retrieval that focuses on the meaning of queries, rather than just matching keywords. It uses natural language processing (NLP) and other advanced techniques to understand the context and intent behind a user's search query, providing more relevant and accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_settings_config = [\n",
    "    SemanticConfiguration(\n",
    "        name=\"config\",\n",
    "        prioritized_fields=PrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"content\"),\n",
    "            prioritized_content_fields=[SemanticField(field_name=\"content\")],\n",
    "            prioritized_keywords_fields=[SemanticField(field_name=\"metadata\")],\n",
    "        ),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 01:55:09,525 - micro - MainProcess - INFO     Azure Cognitive Search client configured successfully. (langchain_integration_azureai.py:setup_azure_search:222)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.azuresearch.AzureSearch at 0x20a9785dca0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the Azure Search client with the specified index\n",
    "# This prepares the client to interact with the Azure Search service\n",
    "gbb_ai_client.setup_azure_search(\n",
    "    index_name=INDEX_NAME,\n",
    "    fields=fields,\n",
    "    semantic_settings_config=semantic_settings_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 01:59:26,727 - micro - MainProcess - INFO     Reading PDF files from C:\\Users\\pablosal\\Desktop\\gbbai-langchain-azureai-search\\utils\\data\\ultraflex_user_manual.pdf. (langchain_integration_azureai.py:read_and_load_pdfs:320)\n",
      "2023-12-21 01:59:51,660 - micro - MainProcess - INFO     Starting to embed and index 39 chuncks. (langchain_integration_azureai.py:embed_and_index:387)\n",
      "2023-12-21 02:00:43,435 - micro - MainProcess - INFO     Successfully embedded and indexed 39 chuncks. (langchain_integration_azureai.py:embed_and_index:389)\n"
     ]
    }
   ],
   "source": [
    "# Scrap web and chuck files intp sentences\n",
    "# Define the URLs of the web pages to scrape\n",
    "file_1 = \"utils\\\\data\\\\ultraflex_user_manual.pdf\"\n",
    "\n",
    "# Set the chunk size and overlap size for splitting the text\n",
    "CHUNK_SIZE = 512\n",
    "OVERLAP_SIZE = 128\n",
    "SEPARATOR = \"(\\n\\w|\\w\\n)\"\n",
    "\n",
    "# Scrape the web pages, split the text into chunks, and store the chunks\n",
    "# The text is split into chunks of size CHUNK_SIZE, with an overlap of OVERLAP_SIZE between consecutive chunks\n",
    "text_chuncked = gbb_ai_client.load_and_split_text_by_character_from_pdf(\n",
    "    source=file_1, chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "\n",
    "# Embed the chunks and index them in Azure Search\n",
    "# This function converts the text chunks into vector embeddings and stores them in the Azure Search index\n",
    "gbb_ai_client.embed_and_index(text_chuncked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-your-own-copilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
