{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Prerequisites\n",
    "\n",
    "Before executing this notebook, make sure you have properly set up your Azure Services, created your Conda environment, and configured your environment variables as per the instructions provided in the [README.md](README.md) file.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "This notebook guides you through the following sections:\n",
    "\n",
    "> **ðŸ’¡ Note:** Please refer to the notebook `01-creation-indexes.ipynb` for detailed information and steps on how to create Azure AI Search Indexes.\n",
    "\n",
    "1. [**Indexing Vectorized Content from Documents**](#index-documents)\n",
    "    - Chunk, vectorize, and index local PDF files and website addresses.\n",
    "    - Download, chunk, vectorize, and index all `.docx` files from a SharePoint site.\n",
    "    - Download PDF files stored in Blob Storage, apply complex OCR processing through GPT-4 Vision, chunk and vectorize the content, and finally index the processed data in Azure AI Search.\n",
    "    \n",
    "2. [**Indexing Vectorized Content from Domanin knowledge document containing complex Documents Images or more**](#index-images)\n",
    "    - Leverage complex OCR, image recognition, and summarization capabilities using GPT-4 Vision. Chunk, vectorize, and index extracted metadata from images stored in Blob Storage.\n",
    "\n",
    "3. [**Indexing Vectorized Content from Audio**](#index-audio)\n",
    "    - Process WAV audio data using Azure AI Speech Translator capabilities, chunk, vectorize, and index audio files stored in Blob Storage and indexed in Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r\"C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\"  # change your directory here\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Azure AI Search Indexes \n",
    "\n",
    "Please refer to the notebook [01-creation-indexes.ipynb](01-creation-indexes.ipynb) for detailed information and steps on how to create Azure AI Search Indexes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Vectorized Content from Multiple Sources and Various Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 18:33:09,098 - micro - MainProcess - INFO     Loading OpenAIEmbeddings object with model, deployment foundational-ada, and chunk size 1000 (ai_search_indexing.py:load_embedding_model:155)\n",
      "c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\vector-indexing-azureaisearch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189: LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.AzureOpenAIEmbeddings instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\vector-indexing-azureaisearch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAIEmbeddings instead.\n",
      "  warn_deprecated(\n",
      "2024-01-14 18:33:10,596 - micro - MainProcess - INFO     AzureOpenAIEmbeddings object has been created successfully. You can now access the embeddings\n",
      "                using the '.embeddings' attribute. (ai_search_indexing.py:load_embedding_model:166)\n",
      "vector_search_configuration is not a known attribute of class <class 'azure.search.documents.indexes.models._index.SearchField'> and will be ignored\n",
      "2024-01-14 18:33:11,851 - micro - MainProcess - INFO     The Azure AI search index 'test-index-002' has been loaded correctly. (ai_search_indexing.py:load_azureai_index:217)\n",
      "2024-01-14 18:33:11,858 - micro - MainProcess - INFO     Successfully loaded environment variables: TENANT_ID, CLIENT_ID, CLIENT_SECRET (sharepoint_data_extractor.py:load_environment_variables_from_env_file:85)\n",
      "2024-01-14 18:33:12,758 - micro - MainProcess - INFO     New access token retrieved. (sharepoint_data_extractor.py:msgraph_auth:118)\n"
     ]
    }
   ],
   "source": [
    "# Import the AzureAIndexer class from the ai_search_indexing module\n",
    "from src.indexers.ai_search_indexing import AzureAIndexer\n",
    "\n",
    "DEPLOYMENT_NAME = \"foundational-ada\"\n",
    "INDEX_NAME = \"test-index-002\"\n",
    "\n",
    "# Create an instance of the AzureAIndexer class\n",
    "azure_search_indexer_client = AzureAIndexer(\n",
    "    index_name=INDEX_NAME, embedding_azure_deployment_name=DEPLOYMENT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Pdfs and Docx from Blob Storage \n",
    "\n",
    "In the process of indexing and processing documents, the first crucial step is loading the files and splitting them into manageable chunks. This is where the `load_files_and_split_into_chunks` function comes into play. This function is designed to streamline these initial steps, preparing your documents for further processing and conversion.\n",
    "\n",
    "Here are its key features:\n",
    "\n",
    "- **Multi-Format Support**: The function can seamlessly process documents in different formats (PDFs, Word documents, etc.) from an array of sources (blob storage, URLs, local paths). You can pass a list of file paths, each possibly in a different format.\n",
    "\n",
    "- **Automated File Loading**: The function efficiently loads files into memory, eliminating the need for manual file handling. It manages the reading and processing of each file.\n",
    "\n",
    "- **Advanced Text Splitting**: After loading, the function splits the text into manageable chunks, which is crucial for processing large documents. You can customize the chunk size and overlap according to your needs.\n",
    "\n",
    "- **Versatile Splitting Options**: You can choose from various splitters - RecursiveCharacterTextSplitter, TokenTextSplitter, SpacyTextSplitter, or CharacterTextSplitter - to fit your specific text processing requirements.\n",
    "\n",
    "- **Encoding Capabilities**: The function can optionally use an encoder during splitting. This feature is particularly useful for certain text analysis tasks. You can specify the model used for encoding (default is \"gpt-4\").\n",
    "\n",
    "- **Verbose Logging**: You can enable detailed logging for in-depth progress tracking and easier debugging.\n",
    "\n",
    "- **High Customizability**: The function's behavior can be tailored to your needs with additional keyword arguments. This includes options like retaining separators in chunks, using separators as regex patterns, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 18:19:46,365 - micro - MainProcess - INFO     Reading .pdf file from local path C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\\utils\\data\\autogen.pdf. (from_blob.py:load_document:66)\n",
      "2024-01-14 18:19:46,367 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_blob.py:load_document:76)\n",
      "2024-01-14 18:19:48,595 - micro - MainProcess - INFO     Reading .pdf file from https://arxiv.org/pdf/2308.08155.pdf. (from_blob.py:load_document:72)\n",
      "2024-01-14 18:19:48,597 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_blob.py:load_document:76)\n",
      "2024-01-14 18:19:52,140 - micro - MainProcess - INFO     Successfully downloaded blob file autogen.pdf (blob_data_extractors.py:extract_content:85)\n",
      "2024-01-14 18:19:52,184 - micro - MainProcess - INFO     Reading .pdf file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpld8pu0uh originally sourced from https://testeastusdev001.blob.core.windows.net/testretrieval/autogen.pdf. (from_blob.py:load_document:62)\n",
      "2024-01-14 18:19:52,187 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_blob.py:load_document:76)\n",
      "2024-01-14 18:19:54,209 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpld8pu0uh (from_blob.py:load_document_from_bytes:129)\n",
      "2024-01-14 18:19:54,210 - micro - MainProcess - INFO     Reading .docx file from local path C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\\utils\\data\\test.docx. (from_blob.py:load_document:66)\n",
      "2024-01-14 18:19:54,211 - micro - MainProcess - INFO     Loading file with Loader Docx2txtLoader (from_blob.py:load_document:76)\n",
      "2024-01-14 18:19:54,456 - micro - MainProcess - INFO     Successfully downloaded blob file test.docx (blob_data_extractors.py:extract_content:85)\n",
      "2024-01-14 18:19:54,495 - micro - MainProcess - INFO     Reading .docx file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmp0kzpnce1 originally sourced from https://testeastusdev001.blob.core.windows.net/testretrieval/test.docx. (from_blob.py:load_document:62)\n",
      "2024-01-14 18:19:54,496 - micro - MainProcess - INFO     Loading file with Loader Docx2txtLoader (from_blob.py:load_document:76)\n",
      "2024-01-14 18:19:54,592 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmp0kzpnce1 (from_blob.py:load_document_from_bytes:129)\n",
      "2024-01-14 18:19:54,595 - micro - MainProcess - INFO     Creating a splitter of type: recursive (by_character.py:get_splitter:57)\n",
      "2024-01-14 18:19:54,598 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (by_character.py:split_documents_in_chunks_from_documents:158)\n",
      "2024-01-14 18:19:54,642 - micro - MainProcess - INFO     Number of chunks obtained: 1408 (by_character.py:split_documents_in_chunks_from_documents:161)\n"
     ]
    }
   ],
   "source": [
    "# Define file paths and URLs\n",
    "local_pdf_path = \"utils/data/autogen.pdf\"\n",
    "remote_pdf_url = \"https://arxiv.org/pdf/2308.08155.pdf\"\n",
    "blob_pdf_url = (\n",
    "    \"https://testeastusdev001.blob.core.windows.net/testretrieval/autogen.pdf\"\n",
    ")\n",
    "local_word_path = \"utils/data/test.docx\"\n",
    "remote_word_url = (\n",
    "    \"https://testeastusdev001.blob.core.windows.net/testretrieval/test.docx\"\n",
    ")\n",
    "\n",
    "# Combine all paths and URLs into a list. This is optional if you want to process multiple files at once.\n",
    "# It will also work by passing a string for simple file processing.\n",
    "file_sources = [\n",
    "    local_pdf_path,\n",
    "    remote_pdf_url,\n",
    "    blob_pdf_url,\n",
    "    local_word_path,\n",
    "    remote_word_url,\n",
    "]\n",
    "\n",
    "# Define parameters for the load_files_and_split_into_chunks function\n",
    "splitter_params = {\n",
    "    \"splitter_type\": \"recursive\",\n",
    "    \"use_encoder\": False,\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 128,\n",
    "    \"verbose\": False,\n",
    "    \"keep_separator\": True,\n",
    "    \"is_separator_regex\": False,\n",
    "    \"model_name\": \"gpt-4\",\n",
    "}\n",
    "\n",
    "# Load files and split them into chunks\n",
    "document_chunks_to_index = azure_search_indexer_client.load_files_and_split_into_chunks(\n",
    "    file_paths=file_sources, **splitter_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 18:20:00,807 - micro - MainProcess - INFO     Embedding and indexing initiated for 1408 text chunks. (ai_search_indexing.py:index_text_embeddings:411)\n",
      "2024-01-14 18:22:17,262 - micro - MainProcess - INFO     Embedding and indexing completed for 1408 text chunks. (ai_search_indexing.py:index_text_embeddings:415)\n"
     ]
    }
   ],
   "source": [
    "# Index the document chunks using the Azure Search Indexer client\n",
    "azure_search_indexer_client.index_text_embeddings(document_chunks_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Pdfs and Docs from Sharepoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"testdocx.docx\", \"autogen.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 18:33:30,476 - micro - MainProcess - INFO     Getting the Site ID... (sharepoint_data_extractor.py:get_site_id:188)\n",
      "2024-01-14 18:33:31,111 - micro - MainProcess - INFO     Site ID retrieved: mngenvmcap747548.sharepoint.com,877fe60f-a62d-4ed1-8eda-af543c437d2c,ac47d8a7-cd54-4344-bd9d-26ada5a075c0 (sharepoint_data_extractor.py:get_site_id:192)\n",
      "2024-01-14 18:33:31,763 - micro - MainProcess - INFO     Successfully retrieved drive ID: b!D-Z_hy2m0U6O2q9UPEN9LKfYR6xUzURDvZ0mraWgdcAot0GWx37EQLiVD3sO7-vm (sharepoint_data_extractor.py:get_drive_id:209)\n",
      "2024-01-14 18:33:31,764 - micro - MainProcess - INFO     Making request to Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:248)\n",
      "2024-01-14 18:33:32,472 - micro - MainProcess - INFO     Received response from Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:251)\n",
      "2024-01-14 18:33:33,832 - micro - MainProcess - INFO     Reading .docx file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpea45kx0b originally sourced from https://mngenvmcap747548.sharepoint.com/sites/Contoso/_layouts/15/Doc.aspx?sourcedoc=%7BE89A926E-0D92-42A8-800D-B8E8DB806BFB%7D&file=testdocx.docx&action=default&mobileredirect=true. (from_sharepoint.py:load_file:113)\n",
      "2024-01-14 18:33:33,834 - micro - MainProcess - INFO     Loading file with Loader Docx2txtLoader (from_sharepoint.py:load_file:127)\n",
      "2024-01-14 18:33:34,011 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpea45kx0b (from_sharepoint.py:load_file_from_bytes:78)\n",
      "2024-01-14 18:33:34,012 - micro - MainProcess - INFO     Getting the Site ID... (sharepoint_data_extractor.py:get_site_id:188)\n",
      "2024-01-14 18:33:34,644 - micro - MainProcess - INFO     Site ID retrieved: mngenvmcap747548.sharepoint.com,877fe60f-a62d-4ed1-8eda-af543c437d2c,ac47d8a7-cd54-4344-bd9d-26ada5a075c0 (sharepoint_data_extractor.py:get_site_id:192)\n",
      "2024-01-14 18:33:35,345 - micro - MainProcess - INFO     Successfully retrieved drive ID: b!D-Z_hy2m0U6O2q9UPEN9LKfYR6xUzURDvZ0mraWgdcAot0GWx37EQLiVD3sO7-vm (sharepoint_data_extractor.py:get_drive_id:209)\n",
      "2024-01-14 18:33:35,347 - micro - MainProcess - INFO     Making request to Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:248)\n",
      "2024-01-14 18:33:36,047 - micro - MainProcess - INFO     Received response from Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:251)\n",
      "2024-01-14 18:33:37,741 - micro - MainProcess - INFO     Reading .pdf file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpleuukazl originally sourced from https://mngenvmcap747548.sharepoint.com/sites/Contoso/Shared%20Documents/autogen.pdf. (from_sharepoint.py:load_file:113)\n",
      "2024-01-14 18:33:37,744 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_sharepoint.py:load_file:127)\n",
      "2024-01-14 18:33:39,917 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpleuukazl (from_sharepoint.py:load_file_from_bytes:78)\n",
      "2024-01-14 18:33:39,919 - micro - MainProcess - INFO     Creating a splitter of type: recursive (by_character.py:get_splitter:57)\n",
      "2024-01-14 18:33:39,920 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (by_character.py:split_documents_in_chunks_from_documents:158)\n",
      "2024-01-14 18:33:39,941 - micro - MainProcess - INFO     Number of chunks obtained: 495 (by_character.py:split_documents_in_chunks_from_documents:161)\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the load_files_and_split_into_chunks function\n",
    "splitter_params = {\n",
    "    \"splitter_type\": \"recursive\",\n",
    "    \"use_encoder\": False,\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 128,\n",
    "    \"verbose\": False,\n",
    "    \"keep_separator\": True,\n",
    "    \"is_separator_regex\": False,\n",
    "    \"model_name\": \"gpt-4\",\n",
    "}\n",
    "\n",
    "document_chunks_to_index = (\n",
    "    azure_search_indexer_client.load_files_and_split_into_chunks_from_sharepoint(\n",
    "        site_domain=os.environ[\"SITE_DOMAIN\"],\n",
    "        site_name=os.environ[\"SITE_NAME\"],\n",
    "        file_names=file_names,\n",
    "        **splitter_params,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 18:34:13,917 - micro - MainProcess - INFO     Embedding and indexing initiated for 495 text chunks. (ai_search_indexing.py:index_text_embeddings:411)\n",
      "2024-01-14 18:35:06,332 - micro - MainProcess - INFO     Embedding and indexing completed for 495 text chunks. (ai_search_indexing.py:index_text_embeddings:415)\n"
     ]
    }
   ],
   "source": [
    "# Index the document chunks using the Azure Search Indexer client\n",
    "azure_search_indexer_client.index_text_embeddings(document_chunks_to_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-your-own-copilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
