{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Prerequisites\n",
    "\n",
    "Before executing this notebook, make sure you have properly set up your Azure Services, created your Conda environment, and configured your environment variables as per the instructions provided in the [README.md](README.md) file.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "This notebook guides you through the following sections:\n",
    "\n",
    "> **ðŸ’¡ Note:** Please refer to the notebook `01-creation-indexes.ipynb` for detailed information and steps on how to create Azure AI Search Indexes.\n",
    "\n",
    "1. [**Indexing Vectorized Content from Documents**](#index-documents)\n",
    "    - Chunk, vectorize, and index local PDF files and website addresses.\n",
    "    - Download, chunk, vectorize, and index all `.docx` files from a SharePoint site.\n",
    "    - Download PDF files stored in Blob Storage, apply complex OCR processing through GPT-4 Vision, chunk and vectorize the content, and finally index the processed data in Azure AI Search.\n",
    "    \n",
    "2. [**Indexing Vectorized Content from complex layout documents laveraging OCR Capabilities**](#index-images)\n",
    "    - Leverage complex OCR, image recognition, and summarization capabilities using Azure Document Intelligence. Chunk, vectorize, and index extracted metadata from Dcouments,\n",
    "\n",
    "3. [**Indexing Vectorized Content from Audio**](#index-audio)\n",
    "    - Process WAV audio data using Azure AI Speech transalations capabilities, chunk, vectorize, and index audio files stored in Blob Storage and indexed in Azure AI Search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r\"C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\"  # change your directory here\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Azure AI Search Indexes \n",
    "\n",
    "Please refer to the notebook [01-creation-indexes.ipynb](01-creation-indexes.ipynb) for detailed information and steps on how to create Azure AI Search Indexes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Vectorized Content from Multiple Sources and Various Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 21:16:09,741 - micro - MainProcess - INFO     Loading OpenAIEmbeddings object with model, deployment foundational-ada, and chunk size 1000 (ai_search_indexing.py:load_embedding_model:158)\n",
      "2024-02-03 21:16:10,493 - micro - MainProcess - INFO     AzureOpenAIEmbeddings object has been created successfully. You can now access the embeddings\n",
      "                using the '.embeddings' attribute. (ai_search_indexing.py:load_embedding_model:169)\n",
      "vector_search_configuration is not a known attribute of class <class 'azure.search.documents.indexes.models._index.SearchField'> and will be ignored\n",
      "2024-02-03 21:16:11,236 - micro - MainProcess - INFO     The Azure AI search index 'test-index-002' has been loaded correctly. (ai_search_indexing.py:load_azureai_index:220)\n",
      "2024-02-03 21:16:11,247 - micro - MainProcess - INFO     Successfully loaded environment variables: TENANT_ID, CLIENT_ID, CLIENT_SECRET (sharepoint_data_extractor.py:load_environment_variables_from_env_file:87)\n",
      "2024-02-03 21:16:11,971 - micro - MainProcess - INFO     New access token retrieved. (sharepoint_data_extractor.py:msgraph_auth:120)\n"
     ]
    }
   ],
   "source": [
    "# Import the AzureAIndexer class from the ai_search_indexing module\n",
    "from src.indexers.ai_search_indexing import AzureAIndexer\n",
    "\n",
    "DEPLOYMENT_NAME = \"foundational-ada\"\n",
    "INDEX_NAME = \"test-index-002\"\n",
    "\n",
    "# Create an instance of the AzureAIndexer class\n",
    "azure_search_indexer_client = AzureAIndexer(\n",
    "    index_name=INDEX_NAME, embedding_azure_deployment_name=DEPLOYMENT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Pdfs and Docx from Blob Storage \n",
    "\n",
    "In the process of indexing and processing documents, the first crucial step is loading the files and splitting them into manageable chunks. This is where the `load_files_and_split_into_chunks` function comes into play. This function is designed to streamline these initial steps, preparing your documents for further processing and conversion.\n",
    "\n",
    "Here are its key features:\n",
    "\n",
    "- **Multi-Format Support**: The function can seamlessly process documents in different formats (PDFs, Word documents, etc.) from an array of sources (blob storage, URLs, local paths). You can pass a list of file paths, each possibly in a different format.\n",
    "\n",
    "- **Automated File Loading**: The function efficiently loads files into memory, eliminating the need for manual file handling. It manages the reading and processing of each file.\n",
    "\n",
    "- **Advanced Text Splitting**: After loading, the function splits the text into manageable chunks, which is crucial for processing large documents. You can customize the chunk size and overlap according to your needs.\n",
    "\n",
    "- **Versatile Splitting Options**: You can choose from various splitters - RecursiveCharacterTextSplitter, TokenTextSplitter, SpacyTextSplitter, or CharacterTextSplitter - to fit your specific text processing requirements.\n",
    "\n",
    "- **Encoding Capabilities**: The function can optionally use an encoder during splitting. This feature is particularly useful for certain text analysis tasks. You can specify the model used for encoding (default is \"gpt-4\").\n",
    "\n",
    "- **Verbose Logging**: You can enable detailed logging for in-depth progress tracking and easier debugging.\n",
    "\n",
    "- **High Customizability**: The function's behavior can be tailored to your needs with additional keyword arguments. This includes options like retaining separators in chunks, using separators as regex patterns, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 21:16:12,043 - micro - MainProcess - INFO     Reading .pdf file from local path C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\\utils\\data\\autogen.pdf. (from_blob.py:load_document:67)\n",
      "2024-02-03 21:16:12,045 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_blob.py:load_document:79)\n",
      "2024-02-03 21:16:14,270 - micro - MainProcess - INFO     Reading .pdf file from https://arxiv.org/pdf/2308.08155.pdf. (from_blob.py:load_document:75)\n",
      "2024-02-03 21:16:14,271 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_blob.py:load_document:79)\n",
      "2024-02-03 21:16:17,521 - micro - MainProcess - INFO     Successfully downloaded blob file autogen.pdf (blob_data_extractors.py:extract_content:89)\n",
      "2024-02-03 21:16:17,558 - micro - MainProcess - INFO     Reading .pdf file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpc9h3k7qm originally sourced from https://testeastusdev001.blob.core.windows.net/testretrieval/autogen.pdf. (from_blob.py:load_document:63)\n",
      "2024-02-03 21:16:17,560 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_blob.py:load_document:79)\n",
      "2024-02-03 21:16:19,394 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpc9h3k7qm (from_blob.py:load_document_from_bytes:132)\n",
      "2024-02-03 21:16:19,395 - micro - MainProcess - INFO     Reading .docx file from local path C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\\utils\\data\\test.docx. (from_blob.py:load_document:67)\n",
      "2024-02-03 21:16:19,396 - micro - MainProcess - INFO     Loading file with Loader Docx2txtLoader (from_blob.py:load_document:79)\n",
      "2024-02-03 21:16:19,551 - micro - MainProcess - INFO     Successfully downloaded blob file test.docx (blob_data_extractors.py:extract_content:89)\n",
      "2024-02-03 21:16:19,593 - micro - MainProcess - INFO     Reading .docx file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmp7fkbj9jb originally sourced from https://testeastusdev001.blob.core.windows.net/testretrieval/test.docx. (from_blob.py:load_document:63)\n",
      "2024-02-03 21:16:19,595 - micro - MainProcess - INFO     Loading file with Loader Docx2txtLoader (from_blob.py:load_document:79)\n",
      "2024-02-03 21:16:19,750 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmp7fkbj9jb (from_blob.py:load_document_from_bytes:132)\n",
      "2024-02-03 21:16:19,752 - micro - MainProcess - INFO     Creating a splitter of type: by_character_recursive (by_character.py:get_splitter:60)\n",
      "2024-02-03 21:16:19,753 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (by_character.py:split_documents_in_chunks_from_documents:161)\n",
      "2024-02-03 21:16:19,799 - micro - MainProcess - INFO     Number of chunks obtained: 1408 (by_character.py:split_documents_in_chunks_from_documents:164)\n"
     ]
    }
   ],
   "source": [
    "# Define file paths and URLs\n",
    "local_pdf_path = \"utils/data/autogen.pdf\"\n",
    "remote_pdf_url = \"https://arxiv.org/pdf/2308.08155.pdf\"\n",
    "blob_pdf_url = (\n",
    "    \"https://testeastusdev001.blob.core.windows.net/testretrieval/autogen.pdf\"\n",
    ")\n",
    "local_word_path = \"utils/data/test.docx\"\n",
    "remote_word_url = (\n",
    "    \"https://testeastusdev001.blob.core.windows.net/testretrieval/test.docx\"\n",
    ")\n",
    "\n",
    "# Combine all paths and URLs into a list. This is optional if you want to process multiple files at once.\n",
    "# It will also work by passing a string for simple file processing.\n",
    "file_sources = [\n",
    "    local_pdf_path,\n",
    "    remote_pdf_url,\n",
    "    blob_pdf_url,\n",
    "    local_word_path,\n",
    "    remote_word_url,\n",
    "]\n",
    "\n",
    "# Define parameters for the load_files_and_split_into_chunks function\n",
    "splitter_params = {\n",
    "    \"splitter_type\": \"by_character_recursive\",\n",
    "    \"use_encoder\": False,\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 128,\n",
    "    \"verbose\": False,\n",
    "    \"keep_separator\": True,\n",
    "    \"is_separator_regex\": False,\n",
    "    \"model_name\": \"gpt-4\",\n",
    "}\n",
    "\n",
    "# Load files and split them into chunks\n",
    "document_chunks_to_index = azure_search_indexer_client.load_files_and_split_into_chunks(\n",
    "    file_paths=file_sources, **splitter_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 21:16:19,836 - micro - MainProcess - INFO     Embedding and indexing initiated for 1408 text chunks. (ai_search_indexing.py:index_text_embeddings:474)\n",
      "2024-02-03 21:18:35,619 - micro - MainProcess - INFO     Embedding and indexing completed for 1408 text chunks. (ai_search_indexing.py:index_text_embeddings:478)\n"
     ]
    }
   ],
   "source": [
    "# Index the document chunks using the Azure Search Indexer client\n",
    "azure_search_indexer_client.index_text_embeddings(document_chunks_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Pdfs and Docs from Sharepoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"testdocx.docx\", \"autogen.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 21:18:35,662 - micro - MainProcess - INFO     Getting the Site ID... (sharepoint_data_extractor.py:get_site_id:190)\n",
      "2024-02-03 21:18:36,299 - micro - MainProcess - INFO     Site ID retrieved: mngenvmcap747548.sharepoint.com,877fe60f-a62d-4ed1-8eda-af543c437d2c,ac47d8a7-cd54-4344-bd9d-26ada5a075c0 (sharepoint_data_extractor.py:get_site_id:194)\n",
      "2024-02-03 21:18:37,022 - micro - MainProcess - INFO     Successfully retrieved drive ID: b!D-Z_hy2m0U6O2q9UPEN9LKfYR6xUzURDvZ0mraWgdcAot0GWx37EQLiVD3sO7-vm (sharepoint_data_extractor.py:get_drive_id:211)\n",
      "2024-02-03 21:18:37,024 - micro - MainProcess - INFO     Making request to Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:250)\n",
      "2024-02-03 21:18:37,669 - micro - MainProcess - INFO     Received response from Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:253)\n",
      "2024-02-03 21:18:38,813 - micro - MainProcess - INFO     Reading .docx file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpdpvzq20h originally sourced from https://mngenvmcap747548.sharepoint.com/sites/Contoso/_layouts/15/Doc.aspx?sourcedoc=%7BE89A926E-0D92-42A8-800D-B8E8DB806BFB%7D&file=testdocx.docx&action=default&mobileredirect=true. (from_sharepoint.py:load_file:111)\n",
      "2024-02-03 21:18:38,814 - micro - MainProcess - INFO     Loading file with Loader Docx2txtLoader (from_sharepoint.py:load_file:127)\n",
      "2024-02-03 21:18:38,894 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpdpvzq20h (from_sharepoint.py:load_file_from_bytes:75)\n",
      "2024-02-03 21:18:38,896 - micro - MainProcess - INFO     Getting the Site ID... (sharepoint_data_extractor.py:get_site_id:190)\n",
      "2024-02-03 21:18:39,585 - micro - MainProcess - INFO     Site ID retrieved: mngenvmcap747548.sharepoint.com,877fe60f-a62d-4ed1-8eda-af543c437d2c,ac47d8a7-cd54-4344-bd9d-26ada5a075c0 (sharepoint_data_extractor.py:get_site_id:194)\n",
      "2024-02-03 21:18:40,263 - micro - MainProcess - INFO     Successfully retrieved drive ID: b!D-Z_hy2m0U6O2q9UPEN9LKfYR6xUzURDvZ0mraWgdcAot0GWx37EQLiVD3sO7-vm (sharepoint_data_extractor.py:get_drive_id:211)\n",
      "2024-02-03 21:18:40,265 - micro - MainProcess - INFO     Making request to Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:250)\n",
      "2024-02-03 21:18:40,919 - micro - MainProcess - INFO     Received response from Microsoft Graph API (sharepoint_data_extractor.py:get_files_in_site:253)\n",
      "2024-02-03 21:18:42,344 - micro - MainProcess - INFO     Reading .pdf file from temporary location C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpulompn4g originally sourced from https://mngenvmcap747548.sharepoint.com/sites/Contoso/Shared%20Documents/autogen.pdf. (from_sharepoint.py:load_file:111)\n",
      "2024-02-03 21:18:42,345 - micro - MainProcess - INFO     Loading file with Loader PyPDFLoader (from_sharepoint.py:load_file:127)\n",
      "2024-02-03 21:18:44,097 - micro - MainProcess - INFO     Deleted temporary file: C:\\Users\\pablosal\\AppData\\Local\\Temp\\tmpulompn4g (from_sharepoint.py:load_file_from_bytes:75)\n",
      "2024-02-03 21:18:44,100 - micro - MainProcess - INFO     Creating a splitter of type: by_character_recursive (by_character.py:get_splitter:60)\n",
      "2024-02-03 21:18:44,103 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (by_character.py:split_documents_in_chunks_from_documents:161)\n",
      "2024-02-03 21:18:44,132 - micro - MainProcess - INFO     Number of chunks obtained: 495 (by_character.py:split_documents_in_chunks_from_documents:164)\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the load_files_and_split_into_chunks function\n",
    "splitter_params = {\n",
    "    \"splitter_type\": \"by_character_recursive\",\n",
    "    \"use_encoder\": False,\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 128,\n",
    "    \"verbose\": False,\n",
    "    \"keep_separator\": True,\n",
    "    \"is_separator_regex\": False,\n",
    "    \"model_name\": \"gpt-4\",\n",
    "}\n",
    "\n",
    "document_chunks_to_index = (\n",
    "    azure_search_indexer_client.load_files_and_split_into_chunks_from_sharepoint(\n",
    "        site_domain=os.environ[\"SITE_DOMAIN\"],\n",
    "        site_name=os.environ[\"SITE_NAME\"],\n",
    "        file_names=file_names,\n",
    "        **splitter_params,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 21:18:44,160 - micro - MainProcess - INFO     Embedding and indexing initiated for 495 text chunks. (ai_search_indexing.py:index_text_embeddings:474)\n",
      "2024-02-03 21:19:32,212 - micro - MainProcess - INFO     Embedding and indexing completed for 495 text chunks. (ai_search_indexing.py:index_text_embeddings:478)\n"
     ]
    }
   ],
   "source": [
    "# Index the document chunks using the Azure Search Indexer client\n",
    "azure_search_indexer_client.index_text_embeddings(document_chunks_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Vectorized Content from complex layout documents laveraging OCR Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_blob = \"https://testeastusdev001.blob.core.windows.net/customskillspdf/instruction-manual-fieldvue-dvc6200-hw2-digital-valve-controller-en-123052.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 21:19:32,249 - micro - MainProcess - INFO     Blob URL detected. Extracting content. (ocr_document_intelligence.py:analyze_document:146)\n",
      "2024-02-03 21:19:33,152 - micro - MainProcess - INFO     Successfully downloaded blob file instruction-manual-fieldvue-dvc6200-hw2-digital-valve-controller-en-123052.pdf (blob_data_extractors.py:extract_content:89)\n",
      "2024-02-03 21:20:05,403 - micro - MainProcess - INFO     Successfully extracted content from https://testeastusdev001.blob.core.windows.net/customskillspdf/instruction-manual-fieldvue-dvc6200-hw2-digital-valve-controller-en-123052.pdf (ocr_data_extractors.py:extract_content:74)\n",
      "2024-02-03 21:20:05,406 - micro - MainProcess - INFO     Section headings: ['# Scope of Manual', '# Conventions Used in this Manual', '# Description', '# Specifications', '# WARNING A', '# Related Documents', '# Table 1-2. Specifications', '# Table 1-2. Specifications (continued)'] (by_title.py:split_text_by_headings:33)\n",
      "2024-02-03 21:20:05,407 - micro - MainProcess - INFO     Number of chunks: 9 (by_title.py:split_text_by_headings:34)\n",
      "2024-02-03 21:20:05,408 - micro - MainProcess - INFO     Processed chunk 1 of 9 (by_title.py:combine_chunks:53)\n",
      "2024-02-03 21:20:05,410 - micro - MainProcess - INFO     Processed chunk 2 of 9 (by_title.py:combine_chunks:53)\n",
      "2024-02-03 21:20:05,411 - micro - MainProcess - INFO     Processed chunk 3 of 9 (by_title.py:combine_chunks:53)\n",
      "2024-02-03 21:20:05,415 - micro - MainProcess - INFO     Processed chunk 4 of 9 (by_title.py:combine_chunks:53)\n",
      "2024-02-03 21:20:05,417 - micro - MainProcess - INFO     Processed chunk 5 of 9 (by_title.py:combine_chunks:53)\n",
      "2024-02-03 21:20:05,419 - micro - MainProcess - INFO     Processed chunk 6 of 9 (by_title.py:combine_chunks:53)\n",
      "2024-02-03 21:20:05,420 - micro - MainProcess - INFO     Processed chunk 7 of 9 (by_title.py:combine_chunks:53)\n",
      "2024-02-03 21:20:05,424 - micro - MainProcess - INFO     Processed chunk 8 of 9 (by_title.py:combine_chunks:53)\n",
      "2024-02-03 21:20:05,428 - micro - MainProcess - INFO     Processed chunk 9 of 9 (by_title.py:combine_chunks:53)\n",
      "2024-02-03 21:20:05,430 - micro - MainProcess - INFO     Processed document 1 of 1 (by_title.py:split_documents_in_chunks_from_documents:90)\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the load_files_and_split_into_chunks function\n",
    "splitter_params = {\n",
    "    \"splitter_type\": \"by_title\",\n",
    "    \"ocr\": True,\n",
    "    \"ocr_output_format\": \"markdown\",\n",
    "    \"pages\": \"3-7\",\n",
    "}\n",
    "\n",
    "document_chunks_to_index = azure_search_indexer_client.load_files_and_split_into_chunks(\n",
    "    file_paths=document_blob,\n",
    "    **splitter_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 21:20:05,449 - micro - MainProcess - INFO     Embedding and indexing initiated for 5 text chunks. (ai_search_indexing.py:index_text_embeddings:474)\n",
      "2024-02-03 21:20:06,240 - micro - MainProcess - INFO     Embedding and indexing completed for 5 text chunks. (ai_search_indexing.py:index_text_embeddings:478)\n"
     ]
    }
   ],
   "source": [
    "# Index the document chunks using the Azure Search Indexer client\n",
    "azure_search_indexer_client.index_text_embeddings(document_chunks_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 21:20:06,254 - micro - MainProcess - INFO     Blob URL detected. Extracting content. (ocr_document_intelligence.py:analyze_document:146)\n",
      "2024-02-03 21:20:06,413 - micro - MainProcess - INFO     Successfully downloaded blob file instruction-manual-fieldvue-dvc6200-hw2-digital-valve-controller-en-123052.pdf (blob_data_extractors.py:extract_content:89)\n",
      "2024-02-03 21:20:37,869 - micro - MainProcess - INFO     Successfully extracted content from https://testeastusdev001.blob.core.windows.net/customskillspdf/instruction-manual-fieldvue-dvc6200-hw2-digital-valve-controller-en-123052.pdf (ocr_data_extractors.py:extract_content:74)\n",
      "2024-02-03 21:20:37,870 - micro - MainProcess - INFO     Creating a splitter of type: by_character_recursive (by_character.py:get_splitter:60)\n",
      "2024-02-03 21:20:37,872 - micro - MainProcess - INFO     Obtained splitter of type: RecursiveCharacterTextSplitter (by_character.py:split_documents_in_chunks_from_documents:161)\n",
      "2024-02-03 21:20:37,875 - micro - MainProcess - INFO     Number of chunks obtained: 38 (by_character.py:split_documents_in_chunks_from_documents:164)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Number: 1, Character Count: 504, Token Count: 107\n",
      "Chunk Number: 2, Character Count: 339, Token Count: 63\n",
      "Chunk Number: 3, Character Count: 362, Token Count: 65\n",
      "Chunk Number: 4, Character Count: 511, Token Count: 96\n",
      "Chunk Number: 5, Character Count: 472, Token Count: 95\n",
      "Chunk Number: 6, Character Count: 510, Token Count: 99\n",
      "Chunk Number: 7, Character Count: 183, Token Count: 36\n",
      "Chunk Number: 8, Character Count: 253, Token Count: 74\n",
      "Chunk Number: 9, Character Count: 408, Token Count: 90\n",
      "Chunk Number: 10, Character Count: 304, Token Count: 55\n",
      "Chunk Number: 11, Character Count: 380, Token Count: 70\n",
      "Chunk Number: 12, Character Count: 494, Token Count: 159\n",
      "Chunk Number: 13, Character Count: 487, Token Count: 142\n",
      "Chunk Number: 14, Character Count: 244, Token Count: 55\n",
      "Chunk Number: 15, Character Count: 502, Token Count: 138\n",
      "Chunk Number: 16, Character Count: 210, Token Count: 62\n",
      "Chunk Number: 17, Character Count: 454, Token Count: 88\n",
      "Chunk Number: 18, Character Count: 456, Token Count: 113\n",
      "Chunk Number: 19, Character Count: 479, Token Count: 123\n",
      "Chunk Number: 20, Character Count: 503, Token Count: 129\n",
      "Chunk Number: 21, Character Count: 376, Token Count: 88\n",
      "Chunk Number: 22, Character Count: 445, Token Count: 105\n",
      "Chunk Number: 23, Character Count: 340, Token Count: 100\n",
      "Chunk Number: 24, Character Count: 505, Token Count: 125\n",
      "Chunk Number: 25, Character Count: 346, Token Count: 89\n",
      "Chunk Number: 26, Character Count: 386, Token Count: 97\n",
      "Chunk Number: 27, Character Count: 401, Token Count: 111\n",
      "Chunk Number: 28, Character Count: 484, Token Count: 164\n",
      "Chunk Number: 29, Character Count: 494, Token Count: 162\n",
      "Chunk Number: 30, Character Count: 391, Token Count: 110\n",
      "Chunk Number: 31, Character Count: 343, Token Count: 69\n",
      "Chunk Number: 32, Character Count: 378, Token Count: 99\n",
      "Chunk Number: 33, Character Count: 478, Token Count: 128\n",
      "Chunk Number: 34, Character Count: 452, Token Count: 112\n",
      "Chunk Number: 35, Character Count: 467, Token Count: 104\n",
      "Chunk Number: 36, Character Count: 460, Token Count: 108\n",
      "Chunk Number: 37, Character Count: 373, Token Count: 121\n",
      "Chunk Number: 38, Character Count: 295, Token Count: 87\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the load_files_and_split_into_chunks function\n",
    "splitter_params = {\n",
    "    \"splitter_type\": \"by_character_recursive\",\n",
    "    \"ocr\": True,\n",
    "    \"ocr_output_format\": \"text\",\n",
    "    \"pages\": \"3-7\",\n",
    "    \"use_encoder\": False,\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 128,\n",
    "    \"verbose\": False,\n",
    "    \"keep_separator\": True,\n",
    "    \"is_separator_regex\": False,\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "document_chunks_to_index = azure_search_indexer_client.load_files_and_split_into_chunks(\n",
    "    file_paths=document_blob,\n",
    "    **splitter_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 21:20:37,899 - micro - MainProcess - INFO     Embedding and indexing initiated for 38 text chunks. (ai_search_indexing.py:index_text_embeddings:474)\n",
      "2024-02-03 21:20:41,377 - micro - MainProcess - INFO     Embedding and indexing completed for 38 text chunks. (ai_search_indexing.py:index_text_embeddings:478)\n"
     ]
    }
   ],
   "source": [
    "# Index the document chunks using the Azure Search Indexer client\n",
    "azure_search_indexer_client.index_text_embeddings(document_chunks_to_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-your-own-copilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
