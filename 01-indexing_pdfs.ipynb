{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùó Problem Statement\n",
    "\n",
    "> üìå **Note**\n",
    ">\n",
    "> This guide is specifically focused on handling PDF documents.\n",
    "\n",
    "When integrating OpenAI models, such as GPT-4, with a vector store, we encounter a unique challenge. This challenge primarily revolves around the process of Retrieval Augmented Generation (RAG). In this process, the model interacts with the vector store to retrieve specific knowledge chunks to answer a particular question from the user. This interaction presents a complex problem for the developer in terms of an effective chunking, sorting, and retrieval data strategy.\n",
    "\n",
    "**üîç The Challenges:**\n",
    "\n",
    "**Data Chunking and Sorting:**\n",
    "\n",
    "+ **Determining Optimal Chunk Size**: Deciding the appropriate size for document chunks is crucial. Too large, and the chunks may exceed the model's context window, leading to loss of information; too small, and they may lack sufficient context.\n",
    "\n",
    "+ **Effective Sorting Strategies**: Sorting these chunks for efficient retrieval is another challenge. The sorting mechanism needs to ensure that the most relevant chunks are prioritized.\n",
    "\n",
    "+ **Overlap Consideration**: Implementing overlapping chunks can be vital. It ensures continuity and context preservation, especially when dealing with long documents or complex topics.\n",
    "\n",
    "**The Impact: Fragmented Information**\n",
    "\n",
    "This fragmentation becomes particularly noticeable when similar terms appear across different sections of a document. The system may inadvertently mix up data from unrelated contexts, leading to potential confusion and misinformation. Additionally, the relevance of retrieved information can vary significantly based on how well the chunking and sorting strategy has been implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Solution\n",
    "\n",
    "Incorporate Azure Search as the vector database, employing an overlapping chunking strategy for enhanced performance using `TextChunkingIndexing`.\n",
    "\n",
    "`TextChunkingIndexing` streamlines the processes of chunking, text vectorization, and indexing within Azure AI Search, using LangChain for enhanced text processing. Discover more about AI Search and LangChain integration [here](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-cognitive-search-and-langchain-a-seamless-integration-for/ba-p/3901448).\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "1. **Text Chunking**: Splits extensive text into manageable chunks for better analysis and indexing.\n",
    "2. **Customization**: Adjusts chunk size and overlap according to different text processing requirements.\n",
    "3. **Text Vectorization**: Converts chunked text into vectors, crucial for effective indexing and retrieval.\n",
    "4. **Indexing in Vector Database**: Stores and retrieves the vectorized text in Azure AI Search.\n",
    "\n",
    "#### Importance of Optimal Chunking\n",
    "\n",
    "Adjusting chunk sizes and overlaps is vital for high-quality text retrieval, especially in precision-based search applications like RAGs. Learn more about fine-tuning and relevance scores [here](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Before you start, ensure you have a `.env` file in your project directory with the following keys:\n",
    "\n",
    "```plaintext\n",
    "OPENAI_API_KEY=****\n",
    "OPENAI_ENDPOINT=****\n",
    "AZURE_OPENAI_API_VERSION=****\n",
    "AZURE_SEARCH_SERVICE_ENDPOINT=****\n",
    "AZURE_SEARCH_ADMIN_KEY=****\n",
    "```\n",
    "\n",
    "#### Setting Up Conda Environment and Configuring VSCode for Jupyter Notebooks\n",
    "\n",
    "Follow these steps to create a Conda environment and set up your VSCode for running Jupyter Notebooks:\n",
    "\n",
    "##### Create Conda Environment from the Repository\n",
    "\n",
    "1. **Prepare the Environment File**:\n",
    "   - Ensure you have an `environment.yml` file in your repository. This file should list all the necessary libraries and dependencies for your project.\n",
    "\n",
    "2. **Use `make` to Create the Conda Environment**:\n",
    "   - In your terminal or command line, navigate to the repository directory and look at the Makefile.\n",
    "   - Execute the `make` command specified below to create the Conda environment using the `environment.yml` file:\n",
    "     \n",
    "     ```bash\n",
    "     make create_conda_env\n",
    "     ```\n",
    "\n",
    "   - This command runs a `make` target that creates a Conda environment as defined in `environment.yml`.\n",
    "\n",
    "3. **Activating the Environment**:\n",
    "   - After creation, activate the new Conda environment by using:\n",
    "     ```bash\n",
    "     conda activate [YourEnvName]\n",
    "     ```\n",
    "     Replace `[YourEnvName]` with the name of your environment as specified in `environment.yml`.\n",
    "\n",
    "##### Configure VSCode for Jupyter Notebooks\n",
    "\n",
    "1. **Install Required Extensions**:\n",
    "   - Download and install the `Python` and `Jupyter` extensions in VSCode.\n",
    "\n",
    "2. **Attach Kernel to VSCode**:\n",
    "   - Once the Conda environment is created, you should be able to see it in the kernel selection (top right corner of your VSCode interface).\n",
    "   - Select your newly created environment as the kernel for running Jupyter Notebooks.\n",
    "\n",
    "By following these steps, you'll set up a dedicated Conda environment for your project and configure VSCode to run Jupyter Notebooks efficiently. This environment will contain all the necessary dependencies in your `environment.yml` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\azure-ai-gbb-solutions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r'C:\\Users\\pablosal\\Desktop\\azure-ai-gbb-solutions' #change your directory here\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 12:37:04,964 - micro - MainProcess - INFO     Loading OpenAIEmbeddings object with model text-embedding-ada-002, deployment foundational-ada, and chunk size 1 (langchain_integration.py:load_embedding_model:106)\n",
      "2023-11-22 12:37:04,968 - micro - MainProcess - INFO     OpenAIEmbeddings object created successfully. (langchain_integration.py:load_embedding_model:119)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, async_client=None, model='text-embedding-ada-002', deployment='foundational-ada', openai_api_version='2023-05-15', openai_api_base='https://ml-workspace-dev-eastus-001-aoai.openai.azure.com/', openai_api_type='azure', openai_proxy='', embedding_ctx_length=8191, openai_api_key='d050ad8b96ef4ecbb5099eece1212a91', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=16, max_retries=2, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=True, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, http_client=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the TextChunkingIndexing class from the langchain_integration module\n",
    "from src.gbb_ai.rag_utils.langchain_integration import TextChunkingIndexing\n",
    "\n",
    "# Create an instance of the TextChunkingIndexing class\n",
    "gbb_ai_client = TextChunkingIndexing()\n",
    "\n",
    "# Set up the OpenAI API client\n",
    "gbb_ai_client.setup_aoai()\n",
    "\n",
    "# Define the name of the deployment\n",
    "DEPLOYMENT_NAME = \"foundational-ada\"\n",
    "\n",
    "# Load the embedding model associated with the specified deployment\n",
    "gbb_ai_client.load_embedding_model(deployment=DEPLOYMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.72it/s]\n",
      "2023-11-22 12:37:06,053 - micro - MainProcess - INFO     Azure Cognitive Search client configured successfully. (langchain_integration.py:setup_azure_search:188)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.azuresearch.AzureSearch at 0x22bb43d8610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the name of the Azure Search index\n",
    "# This is the index where your data is stored in Azure Search\n",
    "INDEX_NAME = \"index-teradyne-web\"\n",
    "\n",
    "# Set up the Azure Search client with the specified index\n",
    "# This prepares the client to interact with the Azure Search service\n",
    "gbb_ai_client.setup_azure_search(index_name=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap web and chuck files intp sentences \n",
    "# Define the URLs of the web pages to scrape\n",
    "file_1 = \"C:\\\\Users\\\\pablosal\\\\Desktop\\\\azure-ai-gbb-solutions\\\\workshop\\\\solution\\\\build_your_own_copilot_aoai\\\\rag_pattern\\\\pdf\\\\ultraflex_user_manual.pdf\"\n",
    "\n",
    "# Set the chunk size and overlap size for splitting the text\n",
    "CHUNK_SIZE = 512\n",
    "OVERLAP_SIZE = 128\n",
    "SEPARATOR = \"(\\n\\w|\\w\\n)\"\n",
    "\n",
    "# Scrape the web pages, split the text into chunks, and store the chunks\n",
    "# The text is split into chunks of size CHUNK_SIZE, with an overlap of OVERLAP_SIZE between consecutive chunks\n",
    "text_chuncked = gbb_ai_client.load_and_split_text_by_character_from_pdf(source=file_1, chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 15.95it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 15.14it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.00it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.84it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.58it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.57it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.47it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.51s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.56it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.26it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.74it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.21it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.12s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.86it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.03it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.15it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.79it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.54it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.44it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.49it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.67it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.76it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.36it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.64it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.94it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.56it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.50it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.43it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.32it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 15.05it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.61it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.07it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.23it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Embed the chunks and index them in Azure Search\n",
    "# This function converts the text chunks into vector embeddings and stores them in the Azure Search index\n",
    "gbb_ai_client.embed_and_index(text_chuncked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-your-own-copilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
