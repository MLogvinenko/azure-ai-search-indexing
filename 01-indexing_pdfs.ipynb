{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Motivation\n",
    "\n",
    "With the intention to harness the full potential of AI technologies like OpenAI's GPT-4, I recognize the need to solve critical challenges in data management and retrieval. This pursuit is driven by the goal to optimize the Retrieval Augmented Generation (RAG) process, ensuring that the AI model not only accesses but also accurately interprets and utilizes the vast knowledge stored in vector databases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùó Problem Statement\n",
    "\n",
    "\n",
    "When integrating OpenAI models, such as GPT-4, with a vector store, we encounter a unique challenge. This challenge primarily revolves around the process of Retrieval Augmented Generation (RAG). In this process, the model interacts with the vector store to retrieve specific knowledge chunks to answer a particular question from the user. This interaction presents a complex problem for the developer in terms of an effective chunking, sorting, and retrieval data strategy.\n",
    "\n",
    "**üîç The Challenges:**\n",
    "\n",
    "**Data Chunking and Sorting:**\n",
    "\n",
    "+ **Determining Optimal Chunk Size**: Deciding the appropriate size for document chunks is crucial. Too large, and the chunks may exceed the model's context window, leading to loss of information; too small, and they may lack sufficient context.\n",
    "\n",
    "+ **Effective Sorting Strategies**: Sorting these chunks for efficient retrieval is another challenge. The sorting mechanism needs to ensure that the most relevant chunks are prioritized.\n",
    "\n",
    "+ **Overlap Consideration**: Implementing overlapping chunks can be vital. It ensures continuity and context preservation, especially when dealing with long documents or complex topics.\n",
    "\n",
    "**The Impact: Fragmented Information**\n",
    "\n",
    "This fragmentation becomes particularly noticeable when similar terms appear across different sections of a document. The system may inadvertently mix up data from unrelated contexts, leading to potential confusion and misinformation. Additionally, the relevance of retrieved information can vary significantly based on how well the chunking and sorting strategy has been implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Solution\n",
    "\n",
    "Optimize chunking strategy to boost Relevant Information Retrieval metrics:\n",
    "\n",
    "The primary goal in this integration is to retrieve the most relevant and contextually accurate information. This requires a nuanced understanding of both the content's nature and the query's intent.\n",
    "\n",
    "**Importance of Chunking Strategy**: Effective chunking strategies, like creating overlapping chunks, play a critical role. They ensure that vital information isn't lost or misrepresented, and that each chunk contains a coherent piece of information that contributes meaningfully to answering the user's query.\n",
    "\n",
    "**Overlapping Chunks for Context Preservation**: Overlapping chunks can bridge the context gap between different sections of a document, ensuring that the retrieved information is not only relevant but also contextually complete, providing a more accurate and holistic response.\n",
    "\n",
    "\"Good to start a.k.a best practices\" -> implies **512 token chunks with 25% overlap.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù How-to\n",
    "\n",
    "Addressing the intricate challenges of retrieval-augmented generation (RAG) systems necessitates a thoughtful and experimental approach. Here are all the elements to enhance your chunking strategy and search:\n",
    "\n",
    "**üöÄ Implement LangChain for high-level orchestration**\n",
    "\n",
    "Use Azure Search as the vector store, incorporating an overlapping chunking strategy.\n",
    "\n",
    "**üîé Exploring Azure Cognitive Search**\n",
    "\n",
    "The query stack in Azure Cognitive Search is structured into two main layers: retrieval (L1) and ranking (L2).\n",
    "\n",
    "+ Retrieval (L1): This layer aims to quickly identify documents from a potentially massive index that meet the search criteria. It operates in three modes:\n",
    "\n",
    "    + Keyword Mode: Utilizes traditional full-text search methods, breaking content into terms through language-specific text analysis, creating inverted indexes for rapid retrieval, and scoring with the BM25 probabilistic model.\n",
    "    - Vector Mode: Converts documents into vector representations using an embedding model (like Azure Open AI's text-embedding-ada-002, or Ada-002) and performs retrieval by matching query embeddings with the closest document vectors.\n",
    "    + Hybrid Mode: Combines keyword and vector retrieval, using Reciprocal Rank Fusion (RRF) to fuse and select the best results from each method.\n",
    "\n",
    "- Ranking (L2): This layer processes a subset of the top results from L1, applying more computational power to compute high-quality relevance scores and reorder them. However, L2 can only enhance the ranking of documents identified by L1; it cannot recover missed documents. The L2 ranking, critical in RAG applications, employs multi-lingual, deep learning models adapted from Microsoft Bing to semantically rank the top 50 L1 results.\n",
    "\n",
    "Azure Cognitive Search's sophisticated approach, combining keyword and vector search in retrieval and enhancing results with advanced semantic ranking, addresses the complexities of large-scale, nuanced search requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Before you start, ensure you have a `.env` file in your project directory with the following keys:\n",
    "\n",
    "```plaintext\n",
    "OPENAI_API_KEY=****\n",
    "OPENAI_ENDPOINT=****\n",
    "AZURE_OPENAI_API_VERSION=****\n",
    "AZURE_SEARCH_SERVICE_ENDPOINT=****\n",
    "AZURE_SEARCH_ADMIN_KEY=****\n",
    "```\n",
    "and install \n",
    "\n",
    "%pip install azure-search-documents==11.4.0b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\azure-ai-gbb-solutions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r'C:\\Users\\pablosal\\Desktop\\azure-ai-gbb-solutions' #change your directory here\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 12:37:04,964 - micro - MainProcess - INFO     Loading OpenAIEmbeddings object with model text-embedding-ada-002, deployment foundational-ada, and chunk size 1 (langchain_integration.py:load_embedding_model:106)\n",
      "2023-11-22 12:37:04,968 - micro - MainProcess - INFO     OpenAIEmbeddings object created successfully. (langchain_integration.py:load_embedding_model:119)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, async_client=None, model='text-embedding-ada-002', deployment='foundational-ada', openai_api_version='2023-05-15', openai_api_base='https://ml-workspace-dev-eastus-001-aoai.openai.azure.com/', openai_api_type='azure', openai_proxy='', embedding_ctx_length=8191, openai_api_key='d050ad8b96ef4ecbb5099eece1212a91', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=16, max_retries=2, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=True, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, http_client=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the TextChunkingIndexing class from the langchain_integration module\n",
    "from src.gbb_ai.rag_utils.langchain_integration import TextChunkingIndexing\n",
    "\n",
    "# Create an instance of the TextChunkingIndexing class\n",
    "gbb_ai_client = TextChunkingIndexing()\n",
    "\n",
    "# Set up the OpenAI API client\n",
    "gbb_ai_client.setup_aoai()\n",
    "\n",
    "# Define the name of the deployment\n",
    "DEPLOYMENT_NAME = \"foundational-ada\"\n",
    "\n",
    "# Load the embedding model associated with the specified deployment\n",
    "gbb_ai_client.load_embedding_model(deployment=DEPLOYMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.72it/s]\n",
      "2023-11-22 12:37:06,053 - micro - MainProcess - INFO     Azure Cognitive Search client configured successfully. (langchain_integration.py:setup_azure_search:188)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.azuresearch.AzureSearch at 0x22bb43d8610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the name of the Azure Search index\n",
    "# This is the index where your data is stored in Azure Search\n",
    "INDEX_NAME = \"index-teradyne-web\"\n",
    "\n",
    "# Set up the Azure Search client with the specified index\n",
    "# This prepares the client to interact with the Azure Search service\n",
    "gbb_ai_client.setup_azure_search(index_name=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap web and chuck files intp sentences \n",
    "# Define the URLs of the web pages to scrape\n",
    "file_1 = \"C:\\\\Users\\\\pablosal\\\\Desktop\\\\azure-ai-gbb-solutions\\\\workshop\\\\solution\\\\build_your_own_copilot_aoai\\\\rag_pattern\\\\pdf\\\\ultraflex_user_manual.pdf\"\n",
    "\n",
    "# Set the chunk size and overlap size for splitting the text\n",
    "CHUNK_SIZE = 512\n",
    "OVERLAP_SIZE = 128\n",
    "SEPARATOR = \"(\\n\\w|\\w\\n)\"\n",
    "\n",
    "# Scrape the web pages, split the text into chunks, and store the chunks\n",
    "# The text is split into chunks of size CHUNK_SIZE, with an overlap of OVERLAP_SIZE between consecutive chunks\n",
    "text_chuncked = gbb_ai_client.load_and_split_text_by_character_from_pdf(source=file_1, chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 15.95it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 15.14it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.00it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.84it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.58it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.57it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.47it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.51s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.56it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.26it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.74it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.21it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.12s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.86it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.03it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.15it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.79it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.54it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.44it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.49it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.67it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.76it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.36it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.64it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.94it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.56it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.50it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.43it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.32it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 15.05it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.61it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.07it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.23it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Embed the chunks and index them in Azure Search\n",
    "# This function converts the text chunks into vector embeddings and stores them in the Azure Search index\n",
    "gbb_ai_client.embed_and_index(text_chuncked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-your-own-copilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
